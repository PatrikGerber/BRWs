\section{N-BRWs WITH POLYNOMIAL TAILS}\label{sec:poly}

\subsection{Overview of heavy tailed distributions}\label{sec:heavy_tailed_overview}
The material in this section relies on sections 2 and 3 of \cite{foss2011introduction}. For a random variable $X$ with cumulative distribution function $F$, let $\overline{F}(x) \defeq \Pr{X > x} = 1 - F(x)$ be its right tail-function. $F$ is said to be heavy-tailed if 
\begin{equation}\label{eqn:heavy_tailed_distr_def}
\int_\R e^{\lambda x} dF = \infty\qquad\forall\, \lambda > 0. 
\end{equation}
$F$ is said to be light-tailed if it is not heavy-tailed. It is clear that if $F$ is positive and light-tailed then it has finite moments of all orders. A positive function $f : \R \to \R_+$ is said to be heavy-tailed if 
\begin{equation}\label{eqn:heavy_tailed_fn_def}
\limsup\limits_{x \to \infty} e^{\lambda x} f(x) = \infty\qquad\forall\,\lambda > 0. 
\end{equation}
We have the following characterisation of heavy tailed distributions:

\begin{theorem}[{{\cite[Theorem 2.6, adapted]{foss2011introduction}}}]
Let $F$ be a distribution function. The following are equivalent:
\begin{itemize}
\item $F$ is heavy-tailed in the sense (\ref{eqn:heavy_tailed_distr_def})
\item $\overline{F}$ is heavy-tailed in the sense (\ref{eqn:heavy_tailed_fn_def}). 
\end{itemize}
If any of the above holds and $F$ has density $f$ with respect to the Lebesgue measure, then $f$ is also heavy-tailed. The converse however is not true in general. 
\end{theorem}

Let $f$ be a positive function as before. For a positive, non-decreasing function $h:\R \to \R_+$ we say that $f$ is $h$-insensitive if 
\begin{equation}\label{eqn:h_insensitive_def}
\lim\limits_{x \to \infty} \sup\limits_{|y| \leq h(x)} \left\lvert \frac{f(x+y)}{f(x)} - 1 \right\rvert = 0. 
\end{equation}
If $f$ is $h$-insensitive for all constant functions $h$ we say that $f$ is long-tailed. For $f$ to be long-tailed, $h$-insensitivity for some $h$ with $h(\infty) = \infty$ is obviously sufficient (and also necessary by Lemma 2.19 \cite{foss2011introduction}). Furthermore $f$ being long-tailed implies that $f$ is heavy-tailed by Lemma 2.17 \cite{foss2011introduction}. By varying the choice of $h$, we can classify long-tailed functions according to the heaviness of their tails. An important subset of long-tailed functions are those of regular variation: A positive function $f:\R \to \R_+$ is said to be regularly varying at $\infty$ with index $\alpha \in \R$ if for all $c > 0$, 
\begin{equation}\nonumber
f(cx) \sim c^\alpha f(x) \qquad\text{ as } x \to \infty. 
\end{equation}
Functions that are $0$-regularly varying are called slowly varying. It is a well known result that if $f$ is $\alpha$-regularly varying then it can be written as $f(x) = x^\alpha f_0(x)$ for some slowly varying $f_0$. The definitions of long-tailedness and regular variation naturally extend to probability distributions $F$: we say $F$ is regularly varying with index $\alpha > 0$ if $\overline{F}$ is regularly varying with index $- \alpha$. A probability distribution having long-tails has a simple probabilistic interpretation: the distribution function $F$ is long-tailed by definition if 
\begin{equation}\label{eqn:long_tail_interpretation}
\PrCond{X > x + y}{X > x} \to 1 \qquad\text{ as } x \to \infty\,, \forall\, y > 0, 
\end{equation}
where $X$ has distribution $F$. Notice that this doesn't give uniform convergence in $y$ on closed intervals as required by definition (\ref{eqn:h_insensitive_def}), however it is a standard result that the two are equivalent. Interpreting (\ref{eqn:long_tail_interpretation}) in words, given that $F$ exceeds some high level the probability of it exceeding an even higher level is large. Finally, we introduce the class of subexponential functions: a distribution $F$ is called subexponential if it is long-tailed and 
\begin{equation}\label{eqn:subexponential_def}
\overline{F * F} (x) \sim 2 \overline{F}(x) \qquad\text{ as } x \to \infty, 
\end{equation}
where $*$ denotes convolution. (\ref{eqn:subexponential_def}) in fact implies $\overline{F^{* (n)}} \sim n \overline{F}$ for $n \geq 1$ so that we have the following probabilistic interpretation: if $(X_i)_{1 \leq i \leq n}$ is an i.i.d. sequence with common distribution $F$ then 
\begin{equation}\nonumber
\Pr{X_1 + ... + X_n > x} \sim \Pr{\max\limits_{1 \leq i \leq n} X_i > x} \qquad\text{ as } x \to \infty. 
\end{equation}
Let $\cal{H}, \cal{L}, \cal{S}, \cal{R}$ denote the class of heavy-tailed, long-tailed, subexponential and regularly varying probability distributions respectively. We have the following inclusions:
\begin{equation}\nonumber
\cal{R} \subset \cal{S} \subset \cal{L} \subset \cal{H}, 
\end{equation}
where the leftmost inclusion is a consequence of Theorem 3.29 \cite{foss2011introduction}. Probability distributions arising in practice virtually always fall in the class of light-tailed distributions or that of the subexponential distributions.  



% Potter's bounds (\cite[Section 6]{poly_tails}) provide a powerful tool for working with regularly varying functions. The result says that for any $C > 1$ and $\delta > 0$ there exist $R \in \R$ such that 
% \begin{equation}
% \frac{f(y)}{f(x)} \leq C \left( \frac{y}{x} \right)^{\alpha + \delta} \lor \left( \frac{y}{x} \right)^{\alpha -\delta} \qquad\forall\, x,y \geq R. 
% \end{equation}





\subsection{The result}\label{sec:poly_result}
Here we present the model as studied in \cite{poly_tails}. Let $(X_n)_{n \geq 0}$ be the $N$-BRW evolving as follows: At each step each particle gives birth to two children whose position is distributed independently and identically around the position of the parent, according to the law of a random variable $X$. Then, out of all children the $N$ rightmost are selected to form the next generation where $N \geq 2$ is fixed. The authors further impose the condition $X \geq 0$ almost surely and that the random variable $X$ be regularly varying with index $\alpha > 0$. The authors remark that the methods of their analysis could be extended to more general reproduction laws and to $X$ taking values in $\R$, however since no new phenomena can be observed the don't do so. For $x \geq 0$ define $h$ to be given by
\begin{equation}\nonumber
\Pr{X > x} \eqdef \frac{1}{h(x)}. 
\end{equation} 
Then our assumptions imply that $h$ is regularly varying at infinity with index $\alpha$. Define the sequence $(c_N)_{N \geq 1}$ by $c_N \defeq h^{-1} (2N \log_2 N)$ where $h^{-1}(x) \defeq \inf\{ y :\, h(y) > x\}$ is the generalised inverse of $h$. Let $\cal{L}(Y)$ denote the law of the random variable $Y$ and let $d(\cdot, \cdot)$ be the Prokhorov metric on the space of probability distributions on $\R$, which induces the topology of weak convergence. We now present Theorem 1.2 of \cite{poly_tails} as it is found there. 

\begin{theorem}[{{\cite[Theorem 1.2]{poly_tails}}}]\label{thm:poly_tails_result}
We distinguish the following cases:
\begin{enumerate}[(a)]

\item \vspace{-2mm} $\alpha > 1$: The limits $v_N \defeq \lim_{n \to \infty} n^{-1} \max X_n = \lim_{n \to \infty} n^{-1} \min X_n$ exist almost surely and in $L^1$ and satisfies $v_N \sim \rho_\alpha c_N / \log_2 N$ as $N \to \infty$ where $\rho_\alpha > 0$. \\

\item \vspace{-5mm} $\alpha = 1,\, \E X < \infty$: $v_N$ exists as above and satisfies 
\begin{equation}\nonumber
v_N \sim \frac{c_N h(c_N)}{\log_2 N} \int\limits^\infty_1 \frac{1}{h(x c_N)} dx \qquad\,\text{as }N \to \infty. \\
\end{equation}

\item \vspace{-5mm} $\alpha = 1,\, \E X = \infty$: Let $b_n = \int\limits^{h^{-1}(n)}_{1} h(c_N)/h(c_N x) dx$. For $i=1$ and $i=N$ we have
\begin{equation}\nonumber
\lim\limits_{N \to \infty} \limsup\limits_{n \to \infty} d(\cal{L}\left( \frac{\log_2 N}{c_N}\frac{X_n(i)}{n b_n}\right), \delta_1) = 0. 
\end{equation}

\item \vspace{-4mm} $\alpha \in (0, 1)$: Let $W_\alpha$ be the random variable whose moment generating function is given by
\begin{equation}\nonumber
\E e^{-\lambda W_\alpha} = exp(- \alpha \int\limits^\infty_0 (1 - e^{-\lambda x})x^{-\alpha - 1}dx). 
\end{equation}
Then for $i=1$ and $i=N$, 
\begin{equation}\nonumber
\lim\limits_{N \to \infty} \limsup\limits_{n \to \infty} d(\cal{L}\left( \frac{1}{(2N)^{1/\alpha}}\frac{X_n(i)}{h^{-1}(n)}\right), \cal{L}(W_\alpha)) = 0. 
\end{equation}
\end{enumerate}
\end{theorem}

The way Bérard and Maillard show this is by considering a stochastic process which they call the `stairs process'. As opposed to the light tailed models considered in Section \ref{sec:light_tails}, when the tails are heavy the cloud of particles moves mainly through large occasional jumps. The stairs process is devised so that it mimics this behaviour. They prove certain properties of these stairs processes, most importantly their asymptotic speed of propagation when suitably scaling space and time, which is the subject of Theorem 2.5 \cite{poly_tails}. Using delicate couplings between the $N$-BRW and the continuous and discretized stairs processes, they obtain that $c_N^{-1} \max X_n$ converges to the stairs process in Skorokhod's $J_1$ topology while the minimum converges in Skorokhod's $M_1$ topology. \\

From Theorem \ref{thm:poly_tails_result} it is hard to tell what $v_N$ looks like in practice because of expressions like $c_N$, $h^{-1}(c_N)$. In Section \ref{sec:examples} we present a worked example as well as simulations which should give us an idea of what these processes look like. From our discussion in Section \ref{sec:heavy_tailed_overview} we know that Bérard and Maillard didn't cover all classes of heavy-tailed distributions, in particular examining what happens for distributions in $\cal{S} \setminus \cal{L}$ is still open.


 % We briefly describe the stairs process to gain some intuition about Bérard and Maillard's argument. \\

% Further define $(\xi_t)_{t \geq 0}$ by $\xi_t = x$ iff $(t, x)$ is an atom of $\cal{P}$ and zero otherwise. 

% Let $\mu_\alpha$ be the measure on $[0, \infty)$ defined by $\mu_\alpha([x, \infty)) = x^{-\alpha}$. Let $\cal{P}$ be a Poisson point process on $\R_+^2$ with intensity $dt \otimes \mu_\alpha$ i.e. let $\cal{P}$ be a random measure on $\R_+^2$ such that for any Borel measurable $G \in \scr{B}(\R_+^2)$ and $n \in \N$ we have $\Pr{\cal{P}(G) = n} = \Pr{\dPo{\int_G dt \otimes \mu_\alpha} = n}$. We will think of the first coordinate as time and of the second as space. The corresponding stairs process $\cal{R}^\alpha \defeq (\cal{R}^\alpha (t))_{t \geq 0}$ is then defined as follows: Suppose that $\cal{R}^\alpha$ is already defined for times $ \leq n$. Now generate the points in the interval $(n, n+1] \times \R_+$ according to $\cal{P}$ and translate every atom $(t,x)$ by $\cal{R}^\alpha (t-1)$ in the space direction. Finally, define $\cal{R}^\alpha$ to be the record process of these points for the time interval $(n, n+1]$. See the introduction of \cite{poly_tails} for two alternate characterisations of $\cal{R}^\alpha$. Bérard and Maillard obtain the following result relating the stairs process and the $N$-BRW with $\alpha$-regularly varying tails:

% \begin{theorem}[{{\cite[Theorem 1.1]{poly_tails}}}]
% With notation as above, 
% \begin{align*}
% (c_N^{-1} \max X(\floor{t \log_2 N)})_{t \geq 0} &\rightarrow (\cal{R}^\alpha (t))_{t \geq 0} \qquad&&\text{ in } J_1, \\ 	
% (c_N^{-1} \max X(\floor{t \log_2 N}), c_N^{-1} \min X(\floor{t \log_2 N}))_{t \geq 0} &\rightarrow (\cal{R}^\alpha (t), \cal{R}^\alpha (t - 1))_{t \geq 0} \qquad&&\text{ in } M_1. 
% \end{align*}
% \end{theorem}

\newpage