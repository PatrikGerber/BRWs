\subsection{Exponentially decaying tails}
\subsubsection{Construction}

The first variation of the $N$-branching random walk that we consider is the one studied in \cite{exp_tails} by Bérard and Gouéré. Suppose that at timestep $n \geq 0$ there is a particle at $x \in \R$. During the branching step the particle dies giving birth to two children, whose positions independently (from each other and the past) follow a distribution with cumulative distribution function $p(\,\cdot - x)$. Out of all $2N$ children, the population at time $n+1$ is then formed by the $N$ rightmost particles. 
\begin{construction} Let $X = (X_n)_{n \geq 0} = (\sum_{i = 1}^N \delta_{X_n(i)})_{n \geq 0}$ denote the $\cal{C}_N$-valued discrete time Markov process defined by the branching-selection procedure detailed above. Note that we suppress the dependence on $N$ in our notation for simplicity. We can construct $X$ easily: Let $\cal{E}_N \defeq (\epsilon_{l, i, j})_{l \geq 0,\,i \in \bbracket{1,N},\,j = 1,2}$ be an i.i.d. collection of random variables distributed like $p$. For $n \geq 0$ define $Y_{n+1} \defeq \sum_{i = 1}^N \sum_{j=1,2} \delta_{X_n(i) + \epsilon_{n, i, j}}$ and take $X_{n+1}$ to be the counting measure supported on the rightmost $N$ atoms of $Y_{n+1}$. This construction gives rise to an important monotonicity property that we record in the following Lemma:
\end{construction}

\begin{lemma}[{{\cite[Corollary 2]{exp_tails}}}]\label{lem:monotonicity}
For any $1 \leq N_1 \leq N_2$ and $\mu_i \in \cal{C}_{N_i}$ with $i=1,2$ such that $\mu_1 \preceq \mu_2$, there exists a coupling $(X^{(1)}_n, X^{(2)}_n)_{n \geq 0}$ between two versions of the branching-selection particle system started from $\mu_1$ and $\mu_2$ respectively satisfying $X^{(1)}_n \preceq X^{(2)}_n$ almost surely for all $n \geq 0$. 
\end{lemma}
\begin{proof}
The proof is a straightworward consequence of our construction. The idea is to take an i.i.d. family $\cal{E}_{N_2} = (\epsilon_{l,i,j})_{l \geq 0,\, i \in \bbracket{1, N_2},\, j = 1,2}$ of random variables with law given by $p$ and to use it to construct both $(X^{(1)})_n$ and $(X^{(2)}_n)_{n \geq 0}$.  
\end{proof}

\subsubsection{Properties of the model}

Let us now define the logarithmic moment generation function of $p$:
\begin{equation*}
\Lambda(t) \defeq \log \int_\R \exp(tx) dp(x). 
\end{equation*}
In their analysis, Bérard and Gouéré impose some conditions on the domain $\cal{D}(\Lambda) \defeq \{t \mid\, \Lambda(t) < \infty \}$ of $\Lambda$ in order for the results of \cite{gantert2008asymptotics} to be applicable. 
\begin{assumption}\label{ass:exponential_tails}
$\Lambda$ is finite in some neighbourhood of $0$. 
\end{assumption}
\begin{assumption}\label{ass:weird}
There exists $t^* > 0$ in the interior of $\cal{D}(\Lambda)$ such that $t^*\Lambda'(t^*) - \Lambda(t^*) = \log 2$. 
\end{assumption}

Assumption \ref{ass:exponential_tails} is in fact equivalent to the requirement that $p$ have exponentially decaying tails, furthermore it implies that $p$ has finite moments of all orders. The results that follow in this section are conditional upon Assumptions \ref{ass:exponential_tails} and \ref{ass:weird} being satisfied. \\

Denote by $\max X_n$ and $\min X_n$ the right- and leftmost atom of $X_n$ respectively. It is worth noting that $\min X_n$ and $\max X_n$ are integrable and hence finite by Assumption \ref{ass:exponential_tails} when started from any fixed $X_0 \in \cal{C}_N$. Denote by $d(X_n) \defeq \max X_n - \min X_n$ the diameter of $X_n$. 

\begin{proposition}[{{\cite[Corollary 1]{exp_tails}}}]\label{prop:diameter}
For any $N \geq 1$ and initial population $X_0 \in \cal{C}_N$, we have 
\begin{equation*}
\frac{d(X_n)}{n} \xrightarrow[n \to \infty]{a.s.,\, L^1} 0. 
\end{equation*}
\end{proposition}

% \begin{proof}
% Set $u_N \defeq \ceil{\frac{\log N}{\log 2}} + 1$ and consider $n \geq u_N$. Recalling the definition of the $\cal{E}_N$ from the construction of $X$, let $\cal{E} \defeq \{ \epsilon_{l, i, j} \mid\, l \in \bbracket{n - u_N,n - 1},\, i \in \bbracket{1, N},\, j = 1,2 \}$ and define $M \defeq \max \cal{E}$ and $m \defeq \min \cal{E}$. Now consider the process $X$ in the timeframe $\bbracket{n - u_N, n}$ where the evolution of $X$ is governed by the random variables in $\cal{E}$. Write $y \defeq \max X_{n - u_N}$ for the front's position at time $n-u_N$. \\
% Suppose for contradiction that for each $k \in \bbracket{1, u_N}$ we have $\min X_{n - u_N + k} < y + k m$. As all steps during branching are $ \geq m$, this implies that all descendents of the particle at position $y$ at time $n - u_N$ are selected during the selection steps until time $n$. As $y$ has exactly $2^{u_N} > N$ descendents at time $n$, we have a contradiction. Therefore it must be that $\min X_{n - u_N + k} \geq y + k_0 m$ for some $k_0$. However, by the definition of $m$ this must also hold for all $k \in \bbracket{k_0, u_N}$, in particular for $k = u_N$. Noting that $\max X_n \leq y + u_N M$, it follows that $d(X_n) \leq u_N (M - m)$ almost surely. \\
% It follows by Assumption \ref{ass:exponential_tails} that $M-m$ has exponentially decaying tails, in particular $\Ex{M - m} < \infty$ so that $n^{-1}\Ex{d(X_n)} \leq n^{-1}\Ex{M - m}$ gives convergence in $L^1$. For any $\epsilon > 0$, exponentially decaying tails also give $\sum_{n \geq u_N} \Pr{n^{-1} d(X_n) > \epsilon} \leq \sum_{n \geq u_N} \Pr{M - m > n \epsilon} < \infty$ so that by the Borel-Cantelli lemma a.s. $n^{-1} d(X_n) > \epsilon$ happens only for finitely many $n$. Almost sure convergence to zero now follows by taking the intersection of these events over $\epsilon \in \Q_+$. 
% \end{proof}

\begin{proposition}[{{\cite[Proposition 2]{exp_tails}}}]\label{prop:ExpTailsSpeedExistence}
There exists $v_N = v_N(p) \in \R$ such that for any initial population $X_0 \in \cal{C}_N$ the following holds almost surely and in $L^1$:
\begin{equation}
\lim\limits_{n \to \infty} \frac{\min X_n}{n} = \lim\limits_{n \to \infty} \frac{\max X_n}{n} = v_N. 
\end{equation}
\end{proposition}

\begin{proof}
Recall the definition of $\cal{E}_N$ from the construction of $X$. For each $l \geq 0$ we define the process $(X^l_n)_{n \geq 0}$ by shifting the origin of time by $l$. More precisely, given the process up to time $n \geq 0$, define $X^l_{n+1}$ to be given by the $N$ rightmost atoms of $\sum_{i = 1}^N \sum_{j=1,2} \delta_{X^l_n(i) + \epsilon_{n + l, i, j}}$. It is clear that each $(X^l_n)_{n \geq 0}$ is distributed as the $N$-branching random walk with offspring law $p$. \\
Suppose that for each $l \geq 0$ we start $(X^l_n)_{n \geq 0}$ from $N \delta_0$ and notice that $(X^0_n)_{n \geq 0} = (X_n)_{n \geq 0}$ almost surely, provided that $X_0 = N \delta_0$ also. From Lemma \ref{lem:monotonicity} it follows easily that 
\begin{align}
\max X^0_{n + m} &\leq \max X^0_n + \max X^n_m && \forall\, n,m \geq 0. 
\end{align}
Provided that the conditions of the theorem hold, applying Kingman's Subadditive Ergodic theorem yields $\lim_{n \to \infty} n^{-1} \max X_n = \lim_{n \to \infty} \Ex{n^{-1} \max X_n} = \inf_n \Ex{n^{-1} \max X_n} = v_N \in \R$ where the first limit is almost sure and also in $L^1$. Using the fact that $p$ has exponentially decaying tails (Assumption \ref{ass:exponential_tails}) and the independence of $\cal{E}_N$, the conditions are easily verified. \\
From Proposition \ref{prop:diameter} we immediately get $\lim_{n \to \infty} n^{-1} \min X_n = v_N$, so the proof is complete in the case $X_0 = N \delta_0$. By translation invariance of the dynamics of the system the result also follows for initial conditions of the form $N \delta_{x_0}$ for any $x_0 \in \R$. Finally, for arbitrary $X_0 \in \cal{C}_N$ note that the result is a consequence of Lemma \ref{lem:monotonicity} and a sandwiching argument between the initial configurations $N \delta_{\min X_0}$ and $N \delta_{\max X_0}$. 
\end{proof}

\begin{proposition}[{{\cite[Proposition 3]{exp_tails}}}]\label{prop:increasing_speed}
The sequence $(v_N)_{N \geq 1}$ is non-decreasing. 
\end{proposition}
\begin{proof}
This is again a consequence of Lemma \ref{lem:monotonicity}. 
\end{proof}

\begin{remark}\label{rem:constants}
From Proposition \ref{prop:increasing_speed} we can deduce that $v_N$ increases to a possibly infinite limit $v_\infty$ as $N$ goes to infinity. Assumption \ref{ass:exponential_tails} implies that $\Lambda$ is smooth on the interior of $\cal{D}(\Lambda)$ so that both quantities $v \defeq \Lambda'(t^*)$ and $\chi \defeq \frac{\pi^2}{2} t^* \Lambda''(t^*)$ are finite. In Section \ref{sec:ExpTails_BrunDer} we will see that $v_\infty$ is in fact equal to $v$. 
\end{remark}

\subsubsection{Killed branching random walks}
Following the notation used in \cite{exp_tails}, we formally define a Branching Random Walk (BRW) to be a pair $(\cal{T}, \Phi)$, where $\cal{T}$ is a rooted binary tree and $\Phi$ is a random map assigning a random variable $\Phi(u)$ to each vertex $u \in \cal{T}$. $\Phi$ must be such that  $\Phi(\text{root}) = 0$ and $\{\Phi(v) - \Phi(u) \mid\, \text{$u$ is the parent of $v$}\}$ is i.i.d. with common distribution $p$. We call $\Phi(u)$ the value of the BRW at vertex $u$ and write $\cal{T}(n)$ for the set of vertices in $\cal{T}$ at distance $n$ from the root. We say a sequence of vertices $u_1, u_2, ...$ is a path if $u_{i+1}$ is the parent of $u_i$ for each $i \geq 1$. \\
Suppose that we have a BRW $(\cal{T}, \Phi)$ and take $v \in \R$ and $m \geq 1$. We say that vertex $u$ is $(m, v)$-good if there exists a path $u = u_0, u_1, ..., u_m$ such that $\Phi(u_i) - \Phi(u) \geq vi$ for all $i \in \bbracket{0,m}$. This is essentially saying that there exists a path started from $u$ that stays to the right of the space-time line through $(u, \Phi(u))$ with slope $v$, for at least $m$ steps. The definition of an $(\infty, v)$-good vertex is analogous. We now state two results from \cite{gantert2008asymptotics} that we will need to prove Theorem \ref{thm:ExpTails_BrunDer}. Recall the definitions of $v$ and $\chi$ from Remark \ref{rem:speeds}. 

\begin{theorem}[{{\cite[Theorem 1.2]{gantert2008asymptotics}}}]\label{thm:infty_good}
Let $\rho(\infty, \epsilon)$ denote the probability that the root of the BRW with offspring distribution $p$ is $(\infty, v - \epsilon)-good$. Then, as $\epsilon > 0$ goes to zero, 
\begin{equation}
\rho(\infty, \epsilon) \leq \exp\left( - \left( \frac{\chi + o(1)}{\epsilon} \right)^{\sfrac{1}{2}} \right). 
\end{equation}
\end{theorem}

A similar result can be stated for the probability of observing a $(m, v - \epsilon)$-good root with $m$ finite:
\begin{theorem}[{{\cite[Theorem 3]{exp_tails}}}]
Let $\rho(m, \epsilon)$ denote the probability that the root of the BRW with offspring distribution $p$ is $(m, v - \epsilon)$-good. For any $0 < \beta < \chi$, there exists $\theta > 0$ such that for all large $m$, 
\begin{equation}\label{thm:finite_good}
\rho(m, \epsilon) \leq \exp\left( - \left( \frac{\chi - \beta}{\epsilon} \right)^{\sfrac{1}{2}} \right), \qquad \text{with } \epsilon \defeq \theta m^{-2/3}. 
\end{equation}
\end{theorem}

\subsubsection{Brunet-Derrida behaviour}\label{sec:ExpTails_BrunDer}

We are now ready to present the main result of Bérard and Gouéré on $N$-branching random walks with exponentially decaying tails:
\begin{theorem}\label{thm:ExpTails_BrunDer}
As $N$ goes to infinity, 
\begin{equation}
v_\infty - v_N = \frac{\chi}{(\log N)^2} + o((\log N)^{-2}). 
\end{equation}
\end{theorem}

Let us describe the coupling between the $N$-branching random walk and $N$ independent branching random walks, which will allow us to relate Theorems \ref{thm:infty_good} and \ref{thm:finite_good} to the $N$-branching random walk. Let $(\text{BRW}_i)_{i \in \bbracket{1,N}} = ((\cal{T}_i, \Phi_i))_{i \in \bbracket{1,N}}$ be a set of $N$ independent copies of the BRW with offspring distribution $p$. Define $\bb{T}_n \defeq \bigsqcup_{i=1}^N \cal{T}_i(n)$ to be the disjoint union of vertices at depth $n$ in the $N$ BRWs, and fix an arbitrary (nonrandom) total order on $\bb{T}_n$ for each $n$. We now inductively define a sequence $(G_n)_{n \geq 0}$ of random subsets of $\bb{T}_n$, each with exactly $N$ elements. These random subsets will correspond to the particles alive in the coupled $N$-braching random walk at time $n$. Define $G_0 = \bb{T}_0$ and given $G_n$, define $H_n$ to be the vertices in $\bb{T}_{n+1}$ that descend from vertices in $G_n$. Finally, set $G_{n+1}$ to be the set of $N$ vertices in $H_n$ with the gratest value, resolving ties via the fixed total order on $\bb{T}_{n+1}$. If we now define (with some abuse of notation) $\frak{X}_n = \sum_{u,i : u \in G_n \cap \cal{T}_i} \delta_{\Phi_i(u)}$ then $(\frak{X}_n)_{n \geq 0}$ has the same distribution as $X$ started from $N \delta_0$. \\
Let us record a technical lemma that will be used in the proof of the lower bound in Theorem \ref{thm:ExpTails_BrunDer}. 

\begin{lemma}[{{\cite[Adapted by Bérard and Gouéré from Lemma 5.2]{pemantle2009search}}}]\label{lem:ExpTails_technical}
Let $v_1 < v_2 \in \R$ and $1 \leq m \leq n \in \N$. Suppose $0 \eqdef x_0, ..., x_n$ is a sequence of real numbers such that $\max_{i \in \bbracket{0, n - 1}} (x_{i+1} - x_i) \leq K$ for some $K > 0$, and define $I \defeq \{ i \in \bbracket{0, n - m} \mid\, x_{i + j} - x_i \geq j v_1,\quad \forall j\in\bbracket{0,m}\}$. If $x_n \geq v_2 n$, then $|I| \geq \frac{v_2 - v_1}{K - v_1}\frac{m}{n} - \frac{K}{K - v_1}$. 
\end{lemma}

\begin{proof}[Proof of lower bound in Theorem \ref{thm:ExpTails_BrunDer}]
As before, set $X_0 = N \delta_0$. Our aim is to show $v_N \defeq \lim_{n \to \infty} \Ex{n^{-1} \max X_n} \leq v_\infty - \chi / (\log N)^2 + o((\log N)^{-2})$. However, we shall show this with $v_\infty$ replaced by $v$, which combined with the upper bound also proves that $v_\infty = v$. Let $\beta \in (0, \chi)$ and let $\theta > 0$ be as in Theorem \ref{thm:finite_good}. Let $\lambda > 0$, and define 
\begin{equation}
m \defeq \ceil*{\theta^{3/2}\left( \frac{(1 + \lambda) \log N}{(\chi - \beta)^{1/2}}\right)^3},
\end{equation}
and $\epsilon \defeq \theta\, m^{-2/3}$. The scale of $\epsilon$ and $m$ is carefully chosen so that by Theorem \ref{thm:finite_good}, 
\begin{equation}
\rho(m, \epsilon) \leq N^{-(1 + \lambda)} \qquad\text{for all large } N. 
\end{equation}
Take $\gamma \in (0, 1)$ and define $v_1 = v - \epsilon$ and $v_2 = v - (1 - \gamma) \epsilon$ noting that $v_1 < v_2 < v$. Finally, let $n = \ceil{N^\xi}$ for some $0 < \xi < \lambda$ and consider the following inequality with $\delta > 0$:
\begin{align}
\Ex{n^{-1} \max X_n} &= \Ex{n^{-1} \max X_n \left[ \Ind_{\{ \max X_n < n v_2\}} + \Ind_{\{ n v_2 \leq \max X_n < n (v + \delta)n \}} + \Ind_{\{ (v + \delta)n \leq \max X_n \}} \right]}\nonumber \\
					&\leq v_2 + (v+\delta)\underbrace{\Pr{\max X_n \leq v_2 n}}_{(I)} + \underbrace{\Ex{n^{-1} \max X_n \Ind_{\{ (v + \delta)n \leq \max X_n \}}}}_{(II)}. \label{ExpTailThmDecomposition}
\end{align}
The strategy for the proof is to show that both $(I)$ and $(II)$ are $o((log N)^{-2})$. The result then follows, as $v_2 = v - (1 - \gamma)(\chi - \beta)(1+\lambda)^{-2}(\log N)^{-2}$ where $\gamma, \beta, \lambda$ can be taken arbitrarily small. \\ 

 Let $B_n$ be the number of vertices in $\sqcup_{i=1}^n G_i$ that are $(m, v_1)$-good with respect to their respective BRWs. Define $K = \kappa \log (2Nn)$ for some $\kappa > 0$ and notice that the quantity $\frac{v_2 - v_1}{K - v_1}\frac{m}{n} - \frac{K}{K - v_1} = \Theta(N^\xi (\log N)^{-4})$ so that for large enough $N$ it is positive. Let $u_0, u_1, ..., u_n$ be a path in $\cal{T}_{i_0}$ for some $i_0 \in \bbracket{1,N}$ such that $u_0 = root_{i_0}$ and $u_n \in G_n$ with $\Phi_{i_0}(u_n) = \max X_n$. In other words, let $u_0, ..., u_n$ be the path from the root to the rightmost particle at time $n$ of the coupled $N$-branching random walk. On the event $E \defeq \{ \max X_n \geq v_2 n \}$, we apply Lemma \ref{lem:ExpTails_technical} to the sequence of real numbers $(\Phi_{i_0}(u_i))_{i \in \bbracket{1,n}}$ to see that either there is an $(m, v_1)$-good vertex among the $u_i$ or one of the random walk steps along the path is $ \geq K$. These events are respectively included in the events that $B_n \geq 1$ and that $M \defeq \max \{\epsilon_{l, i, j} \mid\, l \in \bbracket{0, n - 1},\, i \in \bbracket{1,N},\, j = 1,2 \} \geq K$. We can use this to bound the probability of $E$:
\begin{equation}
\Pr{E} \leq \Pr{M \geq K} + \Pr{B_n \geq 1}. 
\end{equation}
Consider a vertex $u \in \cal{T}_{i_0}(d)$ for some $i_0 \in \bbracket{1,N}$ at depth $d \in \bbracket{0, n}$. The event $\{u \in G_d\}$ is measurable with respect to the sigma algebra generated by the random variables $\{ \Phi_j(v) \mid\, j \in \bbracket{1, N},\,\cal{T}_j \ni v'\text{s depth }\leq d\}$. On the other hand, the event $\{ u \text{ is }(m, v_1) \text{-good}\}$ is determined by the variables $\{ \Phi_{i_0}(v) - \Phi_{i_0}(u) \mid\, \cal{T}_{i_0} \ni v'\text{s depth }> d\}$, so that the two events are independent. We can write $B_n$ as 
\begin{align*}
B_n &= \sum\limits_{i \in \bbracket{1,N},\,u \in \cal{T}_i} \Ind_{\{u \text{ is } (n, v_1)\text{-good}\}} \Ind_{\{u \in G_d \text{ for some d} \in \bbracket{0,n}\}}. \\
\intertext{Taking expectations gives}\label{eqn:B_nOrder}
% \Ex{B_n} &= \sum\limits_{i \in \bbracket{1,N},\,u \in \cal{T}_i} \Pr{u \text{ is } (n, v_1)\text{-good}} \Pr{u \in G_d \text{ for some d} \in \bbracket{0,n}} \\
\Ex{B_n} &\leq N(n+1)\rho(m, \epsilon) = \cal{O}(N^{\xi -\lambda}) = o((\log N)^{-2}) \qquad\text{as $N$ goes to infinity}, 
\end{align*}
where we used that $G_n$ has $N$ elements for all $n$. Recall that the distribution $p$ has exponentially decaying tails, so that there exist $C, \gamma > 0$ such that $\P_{X \sim p}(X > t) \leq C \exp(-\gamma t)$ for all large $t$. This gives $\Pr{M \geq K} \leq 1 - (1 - \exp(- \gamma \kappa \log(2Nn)))^{2Nn} \leq (2Nn)^{1-\gamma \kappa} = o((\log N)^{-2})$, for $\kappa > \gamma^{-1}$. Together with \ref{eqn:B_nOrder} this shows that $(I) = o((\log)^{-2})$. \\
To show that $(II) = o((\log N)^{-2})$ first consider the obvious inequality $\exp(t \max X_n) \leq \sum_{i \in \bbracket{1,N},\, u \in \cal{T}_i(n)} \exp(t \Phi_i(u))$. Taking expectations gives $\Ex{\exp(t \max X_n)} \leq N 2^n \exp(n \Lambda(t))$, where we used a telescoping sum along the path connecting the root and $u$ and the fact that $\# \cal{T}_i(n) = 2^n$ for each $i$. Recalling from Assumption \ref{ass:weird} and Remark \ref{rem:constants} that $\Lambda(t^*) = v t^* - \log 2$, we obtain
\begin{equation} \label{eqn:ExpTailT*bound}
\Ex{\exp(t^* \max X_n)} \leq N \exp( v n t^*).  
\end{equation}
% Let $\alpha \in (0,1)$ be any constant. Notice that for all sufficiently large $x \in \R$, $x \leq \exp(t^* x)$ so that 
% \begin{equation}
% \Ex{(\max X_n - vn) \Ind_{\{ (v + \delta)n \leq \max X_n \}}} \leq \Ex{\exp(t^* ()}
% \end{equation}

\begin{lemma} \label{lem:ExpTailBound}
Let $b > 0$. Then for all large enough $a$, 
\begin{equation}
x \Ind_{\{x \geq a\}} \leq \exp\left(b\left(x - \frac{a}{2}\right)\right), \qquad\forall\, x \in \R. 
\end{equation}
\end{lemma}
\begin{proof}
Differentiate the map $f:x \mapsto \exp(b(x - a/2)) - x$ to find that for large enough $a$, f is increasing on $[a, \infty)$. Noting that $f(a) \geq 0$ for all large $a$ concludes the proof.  
\end{proof}
Apply Lemma \ref{lem:ExpTailBound} with $X = \max X_n - vn$, $a = \delta n, b = t^*$ and take expectations to get 
\begin{align*}
\Ex{(\max X_n - vn) \Ind_{\{\max X_n \geq (v + \delta)n\}}} &\leq \Ex{\exp(t^*(X_n - vn - \delta n / 2))}, \nonumber \\
\intertext{which combined with \ref{eqn:ExpTailT*bound} and a Chernoff bound gives}
(II) = \Ex{\max X_n \Ind_{\{\max X_n \geq (v + \delta)n\}}} &\leq N \exp(-\delta n /2) (1 + |v|n) = o((log N)^{-2}). 
\end{align*}
We have shown that for any choice of $\gamma \in (0,1)$, $\beta \in (0, \chi)$ and $\lambda > \xi > 0$, for all $N$ large enough
\begin{equation}\label{eqn:ExpTailResultTally}
\Ex{\ceil{N^\xi}^{-1} \max X_{\ceil{N^\xi}}} \leq v - (1 - \gamma)\frac{\chi - \beta}{(1 + \lambda)^2(\log N)^2} + o((\log N)^{-2}). 
\end{equation}
Recall from the proof of Proposition \ref{prop:ExpTailsSpeedExistence} that $v_N = \inf_n n^{-1} \Ex{\max X_n}$, so the left hand side in \ref{eqn:ExpTailResultTally} can be replaced by $v_N$. Taking $\gamma, \beta, \lambda$ and $\xi$ to zero gives the desired result. 
\end{proof}